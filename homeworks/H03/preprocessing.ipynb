{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff81fb6",
   "metadata": {},
   "source": [
    "# Creating an ML-Ready Dataset\n",
    "\n",
    "The penguins dataset is a popular dataset for classification tasks. It contains information about different species of penguins, including their physical characteristics and the island they were found on. In this notebook, we will create a machine learning-ready dataset from the penguins dataset.\n",
    "\n",
    "I have made some changes to the original penguins dataset to make our preprocessing more interesting, but the overall structure is the same as can be found online:\n",
    "\n",
    "- bill_length_mm: Bill length (mm) of the penguin.\n",
    "- bill_depth_mm: Bill depth (mm) of the penguin.\n",
    "- flipper_length_mm: Flipper length (mm) of the penguin.\n",
    "- body_mass_g: Body mass (g) of the penguin.\n",
    "- species: Species of the penguin (Adelie, Chinstrap, Gentoo).\n",
    "- island: Island where the penguin was found (Biscoe, Dream)\n",
    "\n",
    "NOTE: This is a cute example, but the principles of data preprocessing are the same for any dataset or domain. The goal is to make the data ready for machine learning algorithms, which require numerical input and clean data.\n",
    "\n",
    "For H.03, we are introducing python scripts which are a great way to organize your code. You will write your functions in the `preprocessing.py` file and then import them into this notebook. This is a good practice for larger projects, as it keeps your code organized and makes it easier to maintain. When you submit your code, you will only need to submit the notebook `preprocessing.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "df = pd.read_csv(\"https://storage.googleapis.com/mbai-data/train_dataset.csv\")\n",
    "NUMERICAL_COLUMNS = [ \"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f5692",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "The first step in any ML Project is to perform Exploratory Data Analysis (EDA). This involves examining the dataset to understand its structure, identify any missing values, and visualize the data. In this case, we will use the `pandas` library to load and explore the dataset.\n",
    "\n",
    "`df.head()` will display the first 5 rows of the dataset, allowing us to see the column names and the first few entries. This is useful for getting a quick overview of the data.\n",
    "\n",
    "`df.describe()` will provide a summary of the dataset, including count, mean, standard deviation, min, max, and quartiles for each numerical column. This is useful for understanding the distribution and range of values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb185be",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8628c92",
   "metadata": {},
   "source": [
    "Looks like we have a dataset with 4 numerical columns and 2 categorical columns. Let's plot the distribution of the numerical columns to see their distributions, since this will have an impact on the methods we choose later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import plot_2x2_histograms\n",
    "\n",
    "fig = plot_2x2_histograms(df, NUMERICAL_COLUMNS)\n",
    "fig.update_layout(title_text=\"Histograms of Penguin Measurements\", showlegend=False, template = \"plotly_white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ba79f",
   "metadata": {},
   "source": [
    "These look approximately normally distributed, so we can use methods that assume normality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d1049",
   "metadata": {},
   "source": [
    "## Identify and Fill Missing Values\n",
    "\n",
    "Looking at the output from `df.head()`, we can immediately see that there are some missing values in the dataset. `ds.info()` provides a summary of the dataset, including the number of non-null values in each column. This can help identify columns with missing values (and their counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6914f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(Markdown(\"------\"))\n",
    "display(df.info())\n",
    "display(Markdown(\"------\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b948a",
   "metadata": {},
   "source": [
    "We can see that all of the numerical columns have **35 missing values**. Let's use an RandomForest imputation to fill in these missing values. In `preprocessing.py`, you will see a function called impute_numerical_values that you should fill out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import impute_numerical_values\n",
    "\n",
    "df[NUMERICAL_COLUMNS] = impute_numerical_values(df[NUMERICAL_COLUMNS].to_numpy())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eb0e7c",
   "metadata": {},
   "source": [
    "That's much better! Now we can see that the missing values have been replaced with estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6ea9f",
   "metadata": {},
   "source": [
    "## Scale Numerical Data\n",
    "\n",
    "There are several ways to scale data. In class, we covered standardization and min-max scaling and the importance of scaling in your machine learning models. In this case, we will implement both using Numpy. We will ultimately use standard scaling for all numerical columns.\n",
    "\n",
    "Please note: This is a good opportunity to learn some basic functionality in numpy (`mean()`, `std()`, `min()`, and `max()`). There are packages that will do scaling for you, but it is important to understand how they work. In this case, we will implement both standardization and min-max scaling using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a81baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import standard_scale_with_numpy, minmax_scale_with_numpy\n",
    "\n",
    "standard_numpy = df.copy()\n",
    "minmax_numpy = df.copy()\n",
    "\n",
    "for feature in NUMERICAL_COLUMNS:\n",
    "    standard_numpy[feature] = standard_scale_with_numpy(df[feature])\n",
    "    minmax_numpy[feature] = minmax_scale_with_numpy(df[feature])\n",
    "\n",
    "display(Markdown(\"#### Standard Scaled Data\"))\n",
    "display(standard_numpy.head())\n",
    "display(Markdown(\"#### MinMax Scaled Data\"))\n",
    "display(minmax_numpy.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e16a62",
   "metadata": {},
   "source": [
    "## Encode Species Variable\n",
    "\n",
    "The species variable is categorical, so we need to encode it as a numerical variable. We will use one-hot encoding to create binary columns for each species. This is a common technique for handling categorical variables in machine learning.\n",
    "\n",
    "Pandas has a built-in function for one-hot encoding, `pd.get_dummies()`, which will create a new column for each unique value in the species column. This will effectively perform one-hot encoding. We will also drop the original species column after encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbf72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import generate_one_hot_encoding\n",
    "\n",
    "df = standard_numpy.copy()\n",
    "df = generate_one_hot_encoding(df)\n",
    "\n",
    "display(Markdown(\"#### One Hot Encoded Data\"))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2b32e",
   "metadata": {},
   "source": [
    "## Create Target Variable\n",
    "\n",
    "Creating a target variable is an important step in preparing your dataset for machine learning. In this case, we will create a target variable called `island` that indicates whether the island is `Biscoe` or `Dream`. This represents a binary classification problem, where we want to predict the island based on the other features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d00494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import binarize_islands\n",
    "\n",
    "df['island'] = binarize_islands(df['island'])\n",
    "\n",
    "display(Markdown(\"#### Binarized Islands\"))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f6193",
   "metadata": {},
   "source": [
    "## Reorder Columns for Convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d916f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import reorder_columns\n",
    "\n",
    "df = reorder_columns(df)\n",
    "display(Markdown(\"#### Reordered Columns\"))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c128b",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49743323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit import send_notebook\n",
    "\n",
    "response = send_notebook(\"./preprocessing.py\")\n",
    "print(response[\"response\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbai-dis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
